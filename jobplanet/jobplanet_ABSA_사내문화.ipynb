{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunnn-y/ICR_Project/blob/main/jobplanet_ABSA_%EC%82%AC%EB%82%B4%EB%AC%B8%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 설정\n",
        "런타임 - 런타임 유형 변경 - GPU"
      ],
      "metadata": {
        "id": "Xkxpf6iOX2yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyVAK0GIX8OZ",
        "outputId": "d281a205-b158-499e-efd8-080515cd9bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 파이썬 버전 확인\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uScnlspBYAdy",
        "outputId": "3c1e79f9-4a17-4ecb-824f-5502a4132604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install mxnet                    # mxnet: 딥러닝 프레임워크\n",
        "!pip install gluonnlp pandas tqdm     # gluonnlp: 오픈소스 딥러닝 기반의 자연어 처리 툴킷\n",
        "!pip install sentencepiece            # sentencepiece: pre-tokenization을 필요로 하지 않는 tokenizer\n",
        "!pip install transformers             # transformer\n",
        "!pip install torch                    # torch"
      ],
      "metadata": {
        "id": "Y5l0nntWYDUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952c1bbd-4a46-4836-f82a-71f74c8b27a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2023.7.22)\n",
            "Requirement already satisfied: gluonnlp in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.23.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/SKTBrain/KoBERT 의 파일을 Colab으로 다운로드\n",
        "!pip install 'git+https://git@github.com/SKTBrain/KoBERT.git@master'\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "W3a3LpXvX--1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 import\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "E1_bOEflYjdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face를 통한 모델 및 토크나이저 Import\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "XBHU7yJgYq0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "APR4fkRJY3u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
      ],
      "metadata": {
        "id": "Vs0dpfz0Y5Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링"
      ],
      "metadata": {
        "id": "44xxTuqoY6hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/ICR_project/잡플래닛_02_학습데이터/jobplanet_labeled_사내문화.csv', index_col=0)"
      ],
      "metadata": {
        "id": "Fh1JZNOZY7TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '감정'이 '중립'인 행 제거\n",
        "df = df[df['감정'] != '중립']"
      ],
      "metadata": {
        "id": "FZJzD2ecVyXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 변경(negative > 0, positive > 1)\n",
        "def changeTo01(x):\n",
        "  if x == '부정':\n",
        "    return 0\n",
        "  elif x == '긍정':\n",
        "    return 1\n",
        "\n",
        "df['감정'] = df['감정'].apply(changeTo01)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xohIJ7_CY897"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [발화문, 상황] data_list 생성\n",
        "data_list = []\n",
        "\n",
        "for review, label in zip(df['문장'], df['감정']):\n",
        "  data = []\n",
        "  data.append(review)\n",
        "  data.append(label)\n",
        "  data_list.append(data)\n",
        "\n",
        "len(data_list)"
      ],
      "metadata": {
        "id": "KtMfAbGpZGNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구축된 데이터셋 확인\n",
        "print(data)\n",
        "print(data_list[:10])"
      ],
      "metadata": {
        "id": "nuwpAkHbg2TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 데이터셋으로 나눔\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 32, stratify=df['감정'])\n",
        "print(len(dataset_train), len(dataset_test))"
      ],
      "metadata": {
        "id": "RDXSmzeXZJkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 토큰화"
      ],
      "metadata": {
        "id": "oeHTiHfahmCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
      ],
      "metadata": {
        "id": "aZjgbojyhioa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch의 Dataset 클래스를 상속받아 BERT 모델을 학습하기 위한 데이터셋을 생성하는 클래스인 BERTDataset을 정의\n",
        "# 이 클래스는 데이터셋을 BERT 모델의 입력 형식으로 변환하는 역할\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "cv_5wNInanez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTDataset 클래스의 파라미터  \n",
        "* dataset: BERT 모델 학습에 사용될 데이터셋\n",
        "*  sent_idx: 문장(텍스트)이 있는 열의 인덱스입니다.\n",
        "* label_idx: 레이블이 있는 열의 인덱스입니다.\n",
        "* bert_tokenizer: BERT 토크나이저입니다.\n",
        "* vocab: 어휘 사전입니다.\n",
        "* max_len: 최대 시퀀스 길이입니다.\n",
        "* pad: 패딩 여부를 나타내는 불리언 값입니다.\n",
        "* pair: 문장 페어(pair) 여부를 나타내는 불리언 값입니다.\n",
        "\n",
        "BERTDataset 클래스의 메서드\n",
        "* __init__(self, ...): 클래스의 초기화 메서드로, 주어진 매개변수들을 사용하여 데이터셋을 BERT 모델의 입력 형식으로 변환할 수 있는 변환 객체를 생성합니다.\n",
        "* __getitem__(self, i): 주어진 인덱스 i에 해당하는 데이터셋의 항목을 반환. 반환되는 항목은 BERT 모델의 입력으로 사용될 수 있도록 변환된 문장과 해당 문장의 레이블\n",
        "* __len__(self): 데이터셋의 전체 길이를 반환"
      ],
      "metadata": {
        "id": "DKwpceBmbTVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "w1Iv3lqzbqDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = tokenizer.tokenize"
      ],
      "metadata": {
        "id": "dwYPg4iacgbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
        "test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)"
      ],
      "metadata": {
        "id": "QXkyuc0PdrQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(test, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "U9ov2IH7dsmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "  def __init__(self, bert, hidden_size=768, num_classes=3, dr_rate=None, params=None):\n",
        "    super(BERTClassifier, self).__init__()\n",
        "    self.bert = bert\n",
        "    self.dr_rate = dr_rate\n",
        "\n",
        "    self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "    if dr_rate:\n",
        "      self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "  def gen_attention_mask(self, token_ids, valid_length):\n",
        "    attention_mask = torch.zeros_like(token_ids)\n",
        "    for i, v in enumerate(valid_length):\n",
        "      attention_mask[i][:v] = 1\n",
        "    return attention_mask.float()\n",
        "\n",
        "  def forward(self, token_ids, valid_length, segment_ids):\n",
        "    attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "    _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "    if self.dr_rate:\n",
        "        out = self.dropout(pooler)\n",
        "    else:\n",
        "        out = pooler\n",
        "    return self.classifier(out)\n"
      ],
      "metadata": {
        "id": "lLrl5vrwduE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
      ],
      "metadata": {
        "id": "J86-HlGudw0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare optimizer and schedule (linear warmup and decay)\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ],
      "metadata": {
        "id": "K9H4c-kxdzWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ],
      "metadata": {
        "id": "v7j548Q9d2EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "CvYylx4jd5IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "BYx-atpNd6lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 state dict만 저장하기\n",
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "category = {'부정':0,'긍정':1}"
      ],
      "metadata": {
        "id": "x_kFQBHJRKrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(predict_sentence):\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "          logits=i\n",
        "          logits = logits.detach().cpu().numpy()\n",
        "          # test_eval.append(list(category.keys())[(np.argmax(logits))]) # 원본 데이터\n",
        "          test_eval.append(logits)\n",
        "        # print(\">> 입력하신 내용은 '\" + test_eval[0] + \"'의 카테고리로 예측되었습니다.\")\n",
        "        probabilities = F.softmax(torch.tensor(test_eval[0]), dim=0).numpy()\n",
        "        # print(\">> 입력하신 내용에 대한 확률 값은:\", probabilities)\n",
        "\n",
        "    return probabilities[1]"
      ],
      "metadata": {
        "id": "3JZ_mwrCbnx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 바꿔야 하는 부분"
      ],
      "metadata": {
        "id": "w-XIriGUBRea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 리뷰 데이터 불러오기\n",
        "corp_name = ['DL이앤씨', 'HDC현대산업개발', '한전KPS', 'HJ중공업', '신세계건설', 'KCC건설', '대우건설', '서희건설', '특수건설', '계룡건설산업', '신원종합개발',\n",
        "             '태영건설', '현대건설', 'GS건설', '동부건설', '코오롱글로벌', 'DL건설', '진흥기업', '남광토건', '한신공영', '금호건설', '삼부토건']\n",
        "\n",
        "for i in corp_name:\n",
        "  review = pd.read_csv(f'/content/drive/MyDrive/ICR_project/잡플래닛_03_실제데이터/jobplanet_{i}_실제데이터.csv')\n",
        "\n",
        "  # ID 칼럼 타입 변환\n",
        "  review['ID'] = review['ID'].astype(object)\n",
        "\n",
        "  # aspect 칼럼 추가\n",
        "  review['구분'] = '사내문화'\n",
        "\n",
        "  # 점수 추출(소수점 세 번째 자리까지 표시)\n",
        "  review['점수'] = review['총평'].apply(predict)\n",
        "  review['점수'] = round(review['점수'],3)\n",
        "\n",
        "  # 최종 데이터 생성\n",
        "  result = review[['ID', '종목명', '분기', '총평','구분', '점수']]\n",
        "  result.head()\n",
        "\n",
        "  # 'jobplanet_기업명_사내문화.csv' 파일로 저장\n",
        "  result.to_csv(f'/content/drive/MyDrive/ICR_project/잡플래닛_04_ABSA결과/004_사내문화/jobplanet_{i}_사내문화.csv', encoding = 'utf-8-sig', index=False)"
      ],
      "metadata": {
        "id": "LkhPgdWvf2nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
